# PEFT (QLoRA) vs Full FT: Stylized Dialogue Generation

Проект демонстрирует дообучение диалоговой модели двумя подходами:
1. Стандартное дообучение **малой языковой модели** ([`rugpt3small_based_on_gpt2`](https://huggingface.co/ai-forever/rugpt3small_based_on_gpt2))
2. Дообучение **большой языковой модели** ([`unsloth/Qwen2.5-7B-Instruct`](https://huggingface.co/unsloth/Qwen2.5-7B-Instruct)) с использованием **параметрически эффективного обучения** (**PEFT**) — методом **QLoRA**

## Цель

1. Обучить модель генерировать короткие стилистически "остроумные" ответы.
2. Минимизировать затраты ресурсов с помощью PEFT.
3. Сравнить дообучение:
    - **большой модели** с PEFT
    - **малой модели** с классическим fine-tuning

## Подходы

### 1. PEFT LLM с помощью QLoRA

- Модель: [`unsloth/Qwen2.5-7B-Instruct`](https://huggingface.co/unsloth/Qwen2.5-7B-Instruct)
- Параметры: `7 млрд.`
- Загрузка в 4-битной квантизации (`load_in_4bit=True`)
- Дообучение: `PEFT с использованием LoRA-адаптера`
  - Параметры LoRA: `r=16`, `lora_alpha=16`, `lora_dropout=0`
- Дообучение с помощью `SFTTrainer`:
  - Параметры обучения: (`max_step=60`, `learning_rate=2e-4`, `epoch=1`, `bf16=True`, `optim=adamw_8bit` — адапт. под квант. модели)
  - Примеров: `30k`
  - Примеры форматированы в соответствии с требованиями Qwen2.5
- Обучение генерации только ответов ассистента: `train_on_responses_only()`

### 2. Full Fine-Tuning небольшой LM

- Модель: [`rugpt3small_based_on_gpt2`](https://huggingface.co/ai-forever/rugpt3small_based_on_gpt2)
- Параметры: `125 млн.`
- Дообучение: `Классический Full-FT`
  - Параметры обучения: (`learning_rate=1e-5`, `num_epochs=4`, `optim=AdamW`)
  - Примеров: `10k` (ограничение по ресурсам)
  - Примеры дополнены специальными токенами (префиксами)

## Датасет

[`igorktech/anekdots_dialogs`](https://huggingface.co/datasets/igorktech/anekdots_dialogs) — коллекция анекдотов, представленных в виде диалогов
- Тип: Диалоговый
- Язык: `Русский`
- Размер: `101k`
- Формат:
  - `original` — полный текст анекдота в "сыром" виде
  - `parsed` — список реплик в формате:<br>`{'speaker_name': 'name', 'phrase': ...}`
  - `total_mark` — общая оценка анекдота

## Метрики
- **BLEU** — n-граммная метрика, измеряет точность совпадений между предсказанием и эталоном.
- **METEOR** — текстовая метрика для оценки смыслового соответствия (учитывает порядок слов, леммы, синонимы, перефразирование)

## Структура

1. **Предобработка датасета** 
- Data Cleaning
  - Отобраны диалоги с двумя репликами
  - Сортировка по убыванию оценки
- Преобразование данных к формату huggingface, для последующего преобразования данных к Qwen2.5 формату.
  - Метод `standartize()`
- Форматирование данных в соответствии с требованиями Qwen2.5:
   Использован формат диалогов в стиле Qwen:
   ```text
   <|im_start|>system
   You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
   <|im_start|>user
   Hello!<|im_end|>
   <|im_start|>assistant
   Hey there! How are you?<|im_end|>
   ```
   - Методы: `get_chat_template()` и `apply_chat_template()`

2. **Исп. базовой модели**
- Загрузка модели
- Добавление QLoRA
- Настройка модели
- Тестирование базовой модели
- Оценка метрик базовой модели

3. **Дообучение**
- Настройка обучения
- Дообучение
- Тестирование обученной модели
- `TextStreamer`
- Оценка метрик дообученной модели

**FULL-FT — АНАЛОГИЧЕН**

## Особенности реализации

- Применён `train_on_responses_only()` — обучение только на ответах ассистента
- Использована совместимость `unsloth` с PEFT и 4-bit квантизацией
- Дополнено оценкой ресурсов

## Сравнение подходов

|    Параметр    | Small LM (rugpt3-samll, full FT)  | LLM (Qwen2.5-7B, PEFT)|
| -------------- | --------------------------------- | ------------------- |
| Обучение       | 20 мин                            | **4 мин**           |
| Кол-во данных  | 10k                               | **30k**             |
| BLEU (до)      | 0.627                             | **0.775**           |
| BLEU (после)   | 0.722                             | **0.818**           |
| METEOR (до)    | 0.468                             | **0.675**           |
| METEOR (после) | 0.634                             | **0.823**           |
| Ответ (до)     | Длинный, нерелевантный            |Длинный, **релевантный**|
|                | без стиля                         | без стиля           |
| Ответ (после)  | Короткий, без стиля               |**Короткий, со стилем**|


## Выводы

1. PEFT даёт доступ к дообучению LLM **в условиях ограниченных ресурсов**
   - В частности QLoRA
   - Квантованная модель сама по себе подходит для инференса
   - Для полноценного дообучения моделей с X млрд. параметраов необходимы огромные ресурсы.
2. PEFT даёт **лучший результат, при меньших затратах**
   - Обучение заняло всего ~4 минуты (вместо 20)
   - Модель достаточной 1й эпохи (вместо 4)
   - Покрыто больше обучающих данных — 30k (вместо 10k)
   - В целом требуется меньше обучающих данных, т.к. LLM уже отлично настроена, а задача сводится к корректированию поведения.
   - [Метрики](#сравнение-подходов) значительно выше
3. PEFT помог модели **полнее понять задачу**
   - Генерирует **короткие** ответы и добавляет юмор
   - smallLM тоже генерирует короткие ответы, но понять стиль сообщений не смогла (или не успела)

LLM научилась выдавать краткие и стилизованные ответы, иногда с юмором, что означает успешное решение задачи.

## Результат

Проект демонстрирует, как с помощью современных методов PEFT и QLoRA можно эффективно адаптировать большие языковые модели к специфическим задачам, даже на ограниченных вычислительных ресурсах. Такой подход превосходит традиционный fine-tuning малых моделей и позволяет быстро обучать мощные LLM под конкретные цели.<br>Модульность: Получен **LoRA-адаптер**, который можно:
  - **Сохранять и переиспользовать**
  - **Динамически подключать/отключать** в зависимости от задачи (например, когда нужны краткие, остроумные ответы)
- Возможна **перезапись полной модели** с интегрированным адаптером (если нужно использовать дообученные веса без LoRA-инфраструктуры)


## Возможные доработки

- Провести более длительное дообучение (увеличить `max_steps`, подобрать learning rate)
- Протестировать другие PEFT-методы (например, AdaLoRA, Prefix Tuning)
- Попробовать более современные или специализированные LLM (например, Mistral, Zephyr)
- Добавить автоматическую генерацию инструкций (Instruction Tuning)
- Реализовать inference-сценарии: API-сервер, интерфейс для тестирования адаптеров в реальном времени

## Как запустить

1. Установите зависимости:
```bash
pip install pandas sklearn torch transformers datasets tqdm torchmetrics evaluate unsloth

# Also get the latest nightly Unsloth!
pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
```

2. Запустите Jupyter Notebook:
```bash
jupyter notebook
```

3. Откройте необходимый ноутбук

4. Аппаратные требования
- Рекомендуется GPU (например, Google Colab с `T4/V100/A100`)
- Объём памяти: от `10 ГБ` и выше для комфортного обучения
